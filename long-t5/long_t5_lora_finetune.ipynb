{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4304f7",
   "metadata": {},
   "source": [
    "# Long-T5 Clinical Query Summarization with LoRA Fine-tuning\n",
    "\n",
    "This notebook fine-tunes the Long-T5 model for medical article summarization using LoRA (Low-Rank Adaptation) for parameter-efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682d819",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdd8e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages installed!\n",
      "Note: Restart the kernel after installation if you haven't already.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "!pip install -q \"numpy<2\"\n",
    "\n",
    "!pip install -q transformers==4.36.0 datasets evaluate accelerate==0.25.0 peft==0.7.1\n",
    "!pip install -q bert-score rouge-score sentencepiece\n",
    "\n",
    "print(\"All packages installed!\")\n",
    "print(\"Note: Restart the kernel after installation if you haven't already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e252324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PyTorch & CUDA Verification\n",
      "==================================================\n",
      "PyTorch version: 2.2.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "Compute capability: 6.0\n",
      "GPU is compatible with this notebook\n",
      "CUDA matrix operations working\n",
      "==================================================\n",
      "CUDA matrix operations working\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"=\" * 50)\n",
    "print(\"PyTorch & CUDA Verification\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    \n",
    "    if capability[0] >= 6:\n",
    "        print(\"GPU is compatible with this notebook\")\n",
    "    else:\n",
    "        print(\"GPU may have limited compatibility\")\n",
    "    \n",
    "    try:\n",
    "        x = torch.randn(100, 100).cuda()\n",
    "        y = torch.matmul(x, x)\n",
    "        print(\"CUDA matrix operations working\")\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA error: {e}\")\n",
    "else:\n",
    "    print(\"Running on CPU - training will be slow\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35e584",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a28a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "GPU Information\n",
      "==================================================\n",
      "CUDA available: True\n",
      "PyTorch version: 2.2.0+cu118\n",
      "CUDA version: 11.8\n",
      "GPU device: Tesla P100-PCIE-16GB\n",
      "GPU memory: 17.06 GB\n",
      "Compute capability: (6, 0)\n",
      "CUDA test: PASSED\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import evaluate\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 50)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA available: True\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Compute capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    try:\n",
    "        test_tensor = torch.tensor([1.0, 2.0, 3.0]).cuda()\n",
    "        _ = test_tensor * 2\n",
    "        print(\"CUDA test: PASSED\")\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA test: FAILED - {e}\")\n",
    "        print(\"Falling back to CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    print(\"CUDA available: False - using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a36e0",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11e39490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "Model: google/long-t5-tglobal-base\n",
      "Max input length: 1536\n",
      "Batch size: 2 (effective: 8)\n",
      "LoRA rank: 16, alpha: 32\n",
      "Note: Settings optimized for P100 (16GB VRAM)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"google/long-t5-tglobal-base\"\n",
    "\n",
    "BASE_DIR = Path(\"/home/cc/clinical-query-summarization/long-t5\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_INPUT_LENGTH = 1536\n",
    "MAX_TARGET_LENGTH = 256\n",
    "GEN_MAX_NEW_TOKENS = 200\n",
    "\n",
    "BATCH_SIZE = 2  \n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 1e-4  \n",
    "NUM_EPOCHS = 5  \n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "EVAL_STEPS = 20\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max input length: {MAX_INPUT_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\n",
    "print(f\"LoRA rank: {LORA_R}, alpha: {LORA_ALPHA}\")\n",
    "print(f\"Note: Settings optimized for P100 (16GB VRAM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ba5df",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4065fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "----------------------------------------\n",
      "Loading data from /home/cc/clinical-query-summarization/long-t5/data/train.json...\n",
      "  Found 392 samples\n",
      "Loading data from /home/cc/clinical-query-summarization/long-t5/data/validation.json...\n",
      "  Found 51 samples\n",
      "Loading data from /home/cc/clinical-query-summarization/long-t5/data/test.json...\n",
      "  Found 109 samples\n",
      "----------------------------------------\n",
      "Total: 392 train, 51 val, 109 test\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_json(json_path):\n",
    "    # Input: article text + question\n",
    "    # Target: answer_abs_summ ----> abstractive summary\n",
    "    print(f\"Loading data from {json_path}...\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    records = []\n",
    "    for qid, payload in raw_data.items():\n",
    "        question = (payload.get('question') or '').strip()\n",
    "        answers = payload.get('answers', {})\n",
    "        \n",
    "        for aid, answer_payload in answers.items():\n",
    "            article = (answer_payload.get('article') or '').strip()\n",
    "            summary = (answer_payload.get('answer_abs_summ') or '').strip()\n",
    "            \n",
    "            if not article or not summary:\n",
    "                continue\n",
    "            \n",
    "            prompt = (\n",
    "                \"Summarize the following medical article to answer the clinical question.\\n\"\n",
    "                f\"Question: {question}\\n\"\n",
    "                f\"Article: {article}\"\n",
    "            )\n",
    "            \n",
    "            records.append({\n",
    "                'id': f'{qid}_{aid}',\n",
    "                'question': question,\n",
    "                'article': article,\n",
    "                'prompt': prompt,\n",
    "                'summary': summary,\n",
    "            })\n",
    "    \n",
    "    print(f\"  Found {len(records)} samples\")\n",
    "    return records\n",
    "\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "print(\"-\" * 40)\n",
    "train_records = load_data_from_json(DATA_DIR / \"train.json\")\n",
    "val_records = load_data_from_json(DATA_DIR / \"validation.json\")\n",
    "test_records = load_data_from_json(DATA_DIR / \"test.json\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total: {len(train_records)} train, {len(val_records)} val, {len(test_records)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81fd7433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training example:\n",
      "============================================================\n",
      "ID: 133_133_Answer2\n",
      "Question: how much oxazepam could cause an overdose?...\n",
      "Article length: 3216 chars\n",
      "Summary: Oxazepam is used to treat anxiety and symptoms of alcohol withdrawal. If you or some you are with overdoses, call your local emergency number, such as 911, or call your local poison center which can b...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training example:\")\n",
    "print(\"=\" * 60)\n",
    "sample = train_records[0]\n",
    "print(f\"ID: {sample['id']}\")\n",
    "print(f\"Question: {sample['question'][:100]}...\")\n",
    "print(f\"Article length: {len(sample['article'])} chars\")\n",
    "print(f\"Summary: {sample['summary'][:200]}...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6b527",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bde6155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from google/long-t5-tglobal-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded! Vocab size: 32100\n",
      "\n",
      "Loading base model from google/long-t5-tglobal-base...\n",
      "This may take a minute...\n",
      "Model loaded!\n",
      "Total parameters: 247,587,456\n",
      "Model loaded!\n",
      "Total parameters: 247,587,456\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded! Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "print(f\"\\nLoading base model from {MODEL_NAME}...\")\n",
    "print(\"This may take a minute...\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in base_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc7d2c",
   "metadata": {},
   "source": [
    "## 6. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92bc556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HuggingFace datasets...\n",
      "Tokenizing datasets...\n",
      "  Tokenizing train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 392/392 [00:00<00:00, 590.15 examples/s]\n",
      "Map: 100%|██████████| 392/392 [00:00<00:00, 590.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 51/51 [00:00<00:00, 426.66 examples/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 109/109 [00:00<00:00, 493.36 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization complete!\n",
      "Train: 392, Val: 51, Test: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['prompt'],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        examples['summary'],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "print(\"Creating HuggingFace datasets...\")\n",
    "train_dataset = Dataset.from_list(train_records)\n",
    "val_dataset = Dataset.from_list(val_records)\n",
    "test_dataset = Dataset.from_list(test_records)\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "print(\"  Tokenizing train set...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "print(\"  Tokenizing validation set...\")\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "print(\"  Tokenizing test set...\")\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "print(f\"\\nTokenization complete!\")\n",
    "print(f\"Train: {len(tokenized_train)}, Val: {len(tokenized_val)}, Test: {len(tokenized_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1cccb",
   "metadata": {},
   "source": [
    "## 7. Baseline Inference (Before Fine-tuning)\n",
    "\n",
    "Generate predictions with the pre-trained Long-T5 model (no fine-tuning) to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e89542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, records, batch_size=2):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    \n",
    "    total_batches = (len(records) + batch_size - 1) // batch_size\n",
    "    print(f\"Generating predictions for {len(records)} samples...\")\n",
    "    \n",
    "    for i in range(0, len(records), batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        if batch_num % 10 == 0 or batch_num == total_batches:\n",
    "            print(f\"  Processing batch {batch_num}/{total_batches}...\")\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            [r['prompt'] for r in batch],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
    "                num_beams=4,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        for record, pred in zip(batch, decoded):\n",
    "            predictions.append({\n",
    "                'id': record['id'],\n",
    "                'question': record['question'],\n",
    "                'reference': record['summary'],\n",
    "                'prediction': pred.strip()\n",
    "            })\n",
    "        \n",
    "        if device.type == 'cuda' and batch_num % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"  Done! Generated {len(predictions)} predictions\")\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def save_predictions(predictions, output_path):\n",
    "    \"\"\"Save predictions to JSON file.\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(f\"Saved predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "982802a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing baseline predictions found.\n",
      "Run the baseline inference cells below to generate them.\n"
     ]
    }
   ],
   "source": [
    "# Load existing baseline predictions\n",
    "# baseline inference cells if predictions already generated and saved\n",
    "\n",
    "baseline_preds_exist = (OUTPUT_DIR / \"baseline_validation_predictions.json\").exists() and \\\n",
    "                       (OUTPUT_DIR / \"baseline_test_predictions.json\").exists()\n",
    "\n",
    "if baseline_preds_exist:\n",
    "    print(\"Loading existing baseline predictions from disk...\")\n",
    "    with open(OUTPUT_DIR / \"baseline_validation_predictions.json\", 'r') as f:\n",
    "        baseline_val_preds = json.load(f)\n",
    "    with open(OUTPUT_DIR / \"baseline_test_predictions.json\", 'r') as f:\n",
    "        baseline_test_preds = json.load(f)\n",
    "    print(f\"  Loaded {len(baseline_val_preds)} validation predictions\")\n",
    "    print(f\"  Loaded {len(baseline_test_preds)} test predictions\")\n",
    "    print(\"Baseline predictions loaded! You can skip the baseline inference cells.\")\n",
    "else:\n",
    "    print(\"No existing baseline predictions found.\")\n",
    "    print(\"Run the baseline inference cells below to generate them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a632c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE INFERENCE (Pre-trained Long-T5, no fine-tuning)\n",
      "============================================================\n",
      "\n",
      "--- Validation Set ---\n",
      "Generating predictions for 51 samples...\n",
      "Generating predictions for 51 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 10/26...\n",
      "  Processing batch 20/26...\n",
      "  Processing batch 20/26...\n",
      "  Processing batch 26/26...\n",
      "  Processing batch 26/26...\n",
      "  Done! Generated 51 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/baseline_validation_predictions.json\n",
      "\n",
      "--- Test Set ---\n",
      "Generating predictions for 109 samples...\n",
      "  Done! Generated 51 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/baseline_validation_predictions.json\n",
      "\n",
      "--- Test Set ---\n",
      "Generating predictions for 109 samples...\n",
      "  Processing batch 10/55...\n",
      "  Processing batch 10/55...\n",
      "  Processing batch 20/55...\n",
      "  Processing batch 20/55...\n",
      "  Processing batch 30/55...\n",
      "  Processing batch 30/55...\n",
      "  Processing batch 40/55...\n",
      "  Processing batch 40/55...\n",
      "  Processing batch 50/55...\n",
      "  Processing batch 50/55...\n",
      "  Processing batch 55/55...\n",
      "  Processing batch 55/55...\n",
      "  Done! Generated 109 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/baseline_test_predictions.json\n",
      "\n",
      "Baseline inference complete!\n",
      "  Done! Generated 109 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/baseline_test_predictions.json\n",
      "\n",
      "Baseline inference complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASELINE INFERENCE (Pre-trained Long-T5, no fine-tuning)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Validation Set ---\")\n",
    "baseline_val_preds = generate_predictions(base_model, val_records, batch_size=2)\n",
    "save_predictions(baseline_val_preds, OUTPUT_DIR / \"baseline_validation_predictions.json\")\n",
    "\n",
    "print(\"\\n--- Test Set ---\")\n",
    "baseline_test_preds = generate_predictions(base_model, test_records, batch_size=2)\n",
    "save_predictions(baseline_test_preds, OUTPUT_DIR / \"baseline_test_predictions.json\")\n",
    "\n",
    "print(\"\\nBaseline inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example baseline prediction:\n",
      "------------------------------------------------------------\n",
      "Question: subjective vertigo Can macular degeneration in only one eye cause dizziness?...\n",
      "\n",
      "Reference: Dizziness has many possible causes, including inner ear disturbance, motion sickness, medication effects, and underlying health condition, such as poor circulation, infection or injury. How long the dizziness lasts and makes you feel, its triggers and other symptoms may help  determine its cause. Yo...\n",
      "\n",
      "Prediction: Symptoms People experiencing dizziness may describe it as any of a number of sensations, such as: - A false sense of motion or spinning (vertigo) - Lightheadedness or feeling faint - Unsteadiness or a loss of balance - A feeling of floating, wooziness or heavy-headedness These feelings may be trigge...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Example baseline prediction:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Question: {baseline_val_preds[0]['question'][:100]}...\")\n",
    "print(f\"\\nReference: {baseline_val_preds[0]['reference'][:300]}...\")\n",
    "print(f\"\\nPrediction: {baseline_val_preds[0]['prediction'][:300]}...\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a9e39",
   "metadata": {},
   "source": [
    "## 8. Apply LoRA for Parameter-Efficient Fine-tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7042f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fresh model for LoRA fine-tuning...\n",
      "\n",
      "Configuring LoRA...\n",
      "\n",
      "Configuring LoRA...\n",
      "\n",
      "============================================================\n",
      "LoRA Configuration Applied!\n",
      "============================================================\n",
      "trainable params: 1,769,472 || all params: 249,356,928 || trainable%: 0.7096141319161583\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "LoRA Configuration Applied!\n",
      "============================================================\n",
      "trainable params: 1,769,472 || all params: 249,356,928 || trainable%: 0.7096141319161583\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading fresh model for LoRA fine-tuning...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"\\nConfiguring LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LoRA Configuration Applied!\")\n",
    "print(\"=\" * 60)\n",
    "model.print_trainable_parameters()\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d58c5b8",
   "metadata": {},
   "source": [
    "## 9. Configure Training with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f590321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 training enabled for P100\n",
      "\n",
      "Training configuration:\n",
      "  Effective batch size: 8\n",
      "  Max epochs: 5\n",
      "  Early stopping patience: 3 evaluations\n",
      "  Evaluation every 20 steps\n",
      "  FP16: True\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "if use_fp16:\n",
    "    print(\"FP16 training enabled for P100\")\n",
    "else:\n",
    "    print(\"Using FP32 training (CPU mode)\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR / \"checkpoints\"),\n",
    "    \n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    \n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=EVAL_STEPS,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    \n",
    "    fp16=use_fp16,\n",
    "    fp16_full_eval=False, \n",
    "    \n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\", \n",
    "    \n",
    "    dataloader_num_workers=0, \n",
    "    dataloader_pin_memory=False,  \n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    early_stopping_threshold=0.0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining configuration:\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE} evaluations\")\n",
    "print(f\"  Evaluation every {EVAL_STEPS} steps\")\n",
    "print(f\"  FP16: {use_fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6d120",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b276fa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized!\n",
      "Training samples: 392\n",
      "Validation samples: 51\n",
      "Steps per epoch: 49\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "print(f\"Training samples: {len(tokenized_train)}\")\n",
    "print(f\"Validation samples: {len(tokenized_val)}\")\n",
    "print(f\"Steps per epoch: {len(tokenized_train) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1f6b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Note: Early stopping will stop training if validation loss\n",
      "      doesn't improve for 3 consecutive evaluations.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 06:55, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>35.782000</td>\n",
       "      <td>40.202431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>33.772500</td>\n",
       "      <td>36.184742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>30.588200</td>\n",
       "      <td>31.780287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>27.496300</td>\n",
       "      <td>28.283424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>26.822600</td>\n",
       "      <td>25.989681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>24.993200</td>\n",
       "      <td>25.188524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE!\n",
      "============================================================\n",
      "Total training time: 419.28 seconds\n",
      "Final training loss: 30.3848\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Note: Early stopping will stop training if validation loss\")\n",
    "print(f\"      doesn't improve for {EARLY_STOPPING_PATIENCE} consecutive evaluations.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter saved to /home/cc/clinical-query-summarization/long-t5/outputs/lora_adapter\n"
     ]
    }
   ],
   "source": [
    "lora_save_path = OUTPUT_DIR / \"lora_adapter\"\n",
    "model.save_pretrained(lora_save_path)\n",
    "print(f\"LoRA adapter saved to {lora_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c41ad",
   "metadata": {},
   "source": [
    "## 11. Fine-tuned Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd81c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINE-TUNED MODEL INFERENCE\n",
      "============================================================\n",
      "\n",
      "--- Validation Set ---\n",
      "Generating predictions for 51 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 10/26...\n",
      "  Processing batch 20/26...\n",
      "  Processing batch 20/26...\n",
      "  Processing batch 26/26...\n",
      "  Processing batch 26/26...\n",
      "  Done! Generated 51 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/finetuned_validation_predictions.json\n",
      "\n",
      "--- Test Set ---\n",
      "Generating predictions for 109 samples...\n",
      "  Done! Generated 51 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/finetuned_validation_predictions.json\n",
      "\n",
      "--- Test Set ---\n",
      "Generating predictions for 109 samples...\n",
      "  Processing batch 10/55...\n",
      "  Processing batch 10/55...\n",
      "  Processing batch 20/55...\n",
      "  Processing batch 20/55...\n",
      "  Processing batch 30/55...\n",
      "  Processing batch 30/55...\n",
      "  Processing batch 40/55...\n",
      "  Processing batch 40/55...\n",
      "  Processing batch 50/55...\n",
      "  Processing batch 50/55...\n",
      "  Processing batch 55/55...\n",
      "  Processing batch 55/55...\n",
      "  Done! Generated 109 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/finetuned_test_predictions.json\n",
      "\n",
      "Fine-tuned inference complete!\n",
      "  Done! Generated 109 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/finetuned_test_predictions.json\n",
      "\n",
      "Fine-tuned inference complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINE-TUNED MODEL INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Validation Set ---\")\n",
    "finetuned_val_preds = generate_predictions(model, val_records, batch_size=2)\n",
    "save_predictions(finetuned_val_preds, OUTPUT_DIR / \"finetuned_validation_predictions.json\")\n",
    "\n",
    "print(\"\\n--- Test Set ---\")\n",
    "finetuned_test_preds = generate_predictions(model, test_records, batch_size=2)\n",
    "save_predictions(finetuned_test_preds, OUTPUT_DIR / \"finetuned_test_predictions.json\")\n",
    "\n",
    "print(\"\\nFine-tuned inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83447b0e",
   "metadata": {},
   "source": [
    "## 12. Evaluation with ROUGE and BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb2d09b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation metrics...\n",
      "Metrics loaded!\n",
      "Metrics loaded!\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "print(\"Loading evaluation metrics...\")\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load('bertscore')\n",
    "print(\"Metrics loaded!\")\n",
    "\n",
    "\n",
    "def compute_metrics(predictions):\n",
    "    refs = [p['reference'] for p in predictions]\n",
    "    preds = [p['prediction'] for p in predictions]\n",
    "    \n",
    "    # ROUGE\n",
    "    rouge_result = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    \n",
    "    # BERTScore\n",
    "    bert_result = bertscore.compute(predictions=preds, references=refs, lang='en')\n",
    "    \n",
    "    metrics = {\n",
    "        'rouge1': round(rouge_result['rouge1'], 4),\n",
    "        'rouge2': round(rouge_result['rouge2'], 4),\n",
    "        'rougeL': round(rouge_result['rougeL'], 4),\n",
    "        'rougeLsum': round(rouge_result['rougeLsum'], 4),\n",
    "        'bertscore_precision': round(np.mean(bert_result['precision']), 4),\n",
    "        'bertscore_recall': round(np.mean(bert_result['recall']), 4),\n",
    "        'bertscore_f1': round(np.mean(bert_result['f1']), 4),\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bb09d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "--- Baseline Model (Validation Set) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rouge1: 0.2595\n",
      "  rouge2: 0.0986\n",
      "  rougeL: 0.1775\n",
      "  rougeLsum: 0.177\n",
      "  bertscore_precision: 0.8208\n",
      "  bertscore_recall: 0.8574\n",
      "  bertscore_f1: 0.8385\n",
      "\n",
      "--- Baseline Model (Test Set) ---\n",
      "  rouge1: 0.2472\n",
      "  rouge2: 0.0884\n",
      "  rougeL: 0.1618\n",
      "  rougeLsum: 0.1618\n",
      "  bertscore_precision: 0.8244\n",
      "  bertscore_recall: 0.8499\n",
      "  bertscore_f1: 0.8366\n",
      "  rouge1: 0.2472\n",
      "  rouge2: 0.0884\n",
      "  rougeL: 0.1618\n",
      "  rougeLsum: 0.1618\n",
      "  bertscore_precision: 0.8244\n",
      "  bertscore_recall: 0.8499\n",
      "  bertscore_f1: 0.8366\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline model\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Baseline Model (Validation Set) ---\")\n",
    "baseline_val_metrics = compute_metrics(baseline_val_preds)\n",
    "for k, v in baseline_val_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n--- Baseline Model (Test Set) ---\")\n",
    "baseline_test_metrics = compute_metrics(baseline_test_preds)\n",
    "for k, v in baseline_test_metrics.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7848d047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuned Model (Validation Set) ---\n",
      "  rouge1: 0.2667\n",
      "  rouge2: 0.0969\n",
      "  rougeL: 0.1786\n",
      "  rougeLsum: 0.179\n",
      "  bertscore_precision: 0.8281\n",
      "  bertscore_recall: 0.8587\n",
      "  bertscore_f1: 0.8429\n",
      "\n",
      "--- Fine-tuned Model (Test Set) ---\n",
      "  rouge1: 0.2667\n",
      "  rouge2: 0.0969\n",
      "  rougeL: 0.1786\n",
      "  rougeLsum: 0.179\n",
      "  bertscore_precision: 0.8281\n",
      "  bertscore_recall: 0.8587\n",
      "  bertscore_f1: 0.8429\n",
      "\n",
      "--- Fine-tuned Model (Test Set) ---\n",
      "  rouge1: 0.2712\n",
      "  rouge2: 0.1053\n",
      "  rougeL: 0.1829\n",
      "  rougeLsum: 0.1835\n",
      "  bertscore_precision: 0.8328\n",
      "  bertscore_recall: 0.856\n",
      "  bertscore_f1: 0.8438\n",
      "  rouge1: 0.2712\n",
      "  rouge2: 0.1053\n",
      "  rougeL: 0.1829\n",
      "  rougeLsum: 0.1835\n",
      "  bertscore_precision: 0.8328\n",
      "  bertscore_recall: 0.856\n",
      "  bertscore_f1: 0.8438\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"\\n--- Fine-tuned Model (Validation Set) ---\")\n",
    "finetuned_val_metrics = compute_metrics(finetuned_val_preds)\n",
    "for k, v in finetuned_val_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n--- Fine-tuned Model (Test Set) ---\")\n",
    "finetuned_test_metrics = compute_metrics(finetuned_test_preds)\n",
    "for k, v in finetuned_test_metrics.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcac96",
   "metadata": {},
   "source": [
    "## 13. Summary Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21a94fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS COMPARISON (Validation Set)\n",
      "================================================================================\n",
      "Metric                           Baseline      Fine-tuned     Improvement\n",
      "--------------------------------------------------------------------------------\n",
      "rouge1                             0.2595          0.2667 +        0.0072\n",
      "rouge2                             0.0986          0.0969        -0.0017\n",
      "rougeL                             0.1775          0.1786 +        0.0011\n",
      "rougeLsum                          0.1770          0.1790 +        0.0020\n",
      "bertscore_f1                       0.8385          0.8429 +        0.0044\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS COMPARISON (Test Set)\n",
      "================================================================================\n",
      "Metric                           Baseline      Fine-tuned     Improvement\n",
      "--------------------------------------------------------------------------------\n",
      "rouge1                             0.2472          0.2712 +        0.0240\n",
      "rouge2                             0.0884          0.1053 +        0.0169\n",
      "rougeL                             0.1618          0.1829 +        0.0211\n",
      "rougeLsum                          0.1618          0.1835 +        0.0217\n",
      "bertscore_f1                       0.8366          0.8438 +        0.0072\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS COMPARISON (Validation Set)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Metric':<25} {'Baseline':>15} {'Fine-tuned':>15} {'Improvement':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'bertscore_f1']:\n",
    "    baseline_val = baseline_val_metrics[metric]\n",
    "    finetuned_val = finetuned_val_metrics[metric]\n",
    "    improvement = finetuned_val - baseline_val\n",
    "    sign = \"+\" if improvement > 0 else \"\"\n",
    "    print(f\"{metric:<25} {baseline_val:>15.4f} {finetuned_val:>15.4f} {sign}{improvement:>14.4f}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS COMPARISON (Test Set)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Metric':<25} {'Baseline':>15} {'Fine-tuned':>15} {'Improvement':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'bertscore_f1']:\n",
    "    baseline_val = baseline_test_metrics[metric]\n",
    "    finetuned_val = finetuned_test_metrics[metric]\n",
    "    improvement = finetuned_val - baseline_val\n",
    "    sign = \"+\" if improvement > 0 else \"\"\n",
    "    print(f\"{metric:<25} {baseline_val:>15.4f} {finetuned_val:>15.4f} {sign}{improvement:>14.4f}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e85aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All metrics saved to /home/cc/clinical-query-summarization/long-t5/outputs/evaluation_metrics.json\n"
     ]
    }
   ],
   "source": [
    "all_metrics = {\n",
    "    'baseline_validation': baseline_val_metrics,\n",
    "    'baseline_test': baseline_test_metrics,\n",
    "    'finetuned_validation': finetuned_val_metrics,\n",
    "    'finetuned_test': finetuned_test_metrics,\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / \"evaluation_metrics.json\", 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "print(f\"\\nAll metrics saved to {OUTPUT_DIR / 'evaluation_metrics.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
