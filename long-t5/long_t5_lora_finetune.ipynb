{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4304f7",
   "metadata": {},
   "source": [
    "# Long-T5 Clinical Query Summarization with LoRA Fine-tuning\n",
    "\n",
    "This notebook fine-tunes the Long-T5 model for medical article summarization using LoRA (Low-Rank Adaptation) for parameter-efficient training.\n",
    "\n",
    "**Features:**\n",
    "- LoRA fine-tuning (~0.5% trainable parameters)\n",
    "- Early stopping to prevent overfitting\n",
    "- ROUGE and BERTScore evaluation\n",
    "- Baseline vs fine-tuned comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682d819",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd8e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages installed!\n",
      "Note: Restart the kernel after installation if you haven't already.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for P100 GPU (CUDA 11.8 compatible)\n",
    "# P100 has compute capability 6.0\n",
    "\n",
    "# PyTorch 2.2.0 with CUDA 11.8 (compatible with P100)\n",
    "# Note: If you already ran the terminal commands to install these, skip this cell\n",
    "!pip install -q torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# NumPy 1.x (required for PyTorch 2.2.0 compatibility)\n",
    "!pip install -q \"numpy<2\"\n",
    "\n",
    "# Install HuggingFace libraries (compatible versions)\n",
    "!pip install -q transformers==4.36.0 datasets evaluate accelerate==0.25.0 peft==0.7.1\n",
    "!pip install -q bert-score rouge-score sentencepiece\n",
    "\n",
    "print(\"All packages installed!\")\n",
    "print(\"Note: Restart the kernel after installation if you haven't already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e252324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PyTorch & CUDA Verification\n",
      "==================================================\n",
      "PyTorch version: 2.2.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "Compute capability: 6.0\n",
      "✓ GPU is compatible with this notebook\n",
      "✓ CUDA matrix operations working\n",
      "==================================================\n",
      "✓ CUDA matrix operations working\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify CUDA and PyTorch compatibility with P100\n",
    "# Run this after restarting the kernel post-installation\n",
    "\n",
    "import torch\n",
    "print(\"=\" * 50)\n",
    "print(\"PyTorch & CUDA Verification\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    \n",
    "    # P100 has compute capability 6.0\n",
    "    if capability[0] >= 6:\n",
    "        print(\"✓ GPU is compatible with this notebook\")\n",
    "    else:\n",
    "        print(\"⚠ GPU may have limited compatibility\")\n",
    "    \n",
    "    # Test CUDA operations\n",
    "    try:\n",
    "        x = torch.randn(100, 100).cuda()\n",
    "        y = torch.matmul(x, x)\n",
    "        print(\"✓ CUDA matrix operations working\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ CUDA error: {e}\")\n",
    "else:\n",
    "    print(\"Running on CPU - training will be slow\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35e584",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a28a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "GPU Information\n",
      "==================================================\n",
      "CUDA available: True\n",
      "PyTorch version: 2.2.0+cu118\n",
      "CUDA version: 11.8\n",
      "GPU device: Tesla P100-PCIE-16GB\n",
      "GPU memory: 17.06 GB\n",
      "Compute capability: (6, 0)\n",
      "CUDA test: PASSED\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import evaluate\n",
    "\n",
    "# Check if GPU is available and verify P100 compatibility\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 50)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA available: True\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Compute capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Quick test to verify CUDA works\n",
    "    try:\n",
    "        test_tensor = torch.tensor([1.0, 2.0, 3.0]).cuda()\n",
    "        _ = test_tensor * 2\n",
    "        print(\"CUDA test: PASSED\")\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA test: FAILED - {e}\")\n",
    "        print(\"Falling back to CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    print(\"CUDA available: False - using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a36e0",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e39490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "Model: google/long-t5-tglobal-base\n",
      "Max input length: 1536\n",
      "Batch size: 2 (effective: 8)\n",
      "LoRA rank: 16, alpha: 32\n",
      "Note: Settings optimized for P100 (16GB VRAM)\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"google/long-t5-tglobal-base\"\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\"/home/cc/clinical-query-summarization/long-t5\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tokenization settings (reduced for P100 16GB memory)\n",
    "MAX_INPUT_LENGTH = 1536  # Reduced from 2048 for P100 memory constraints\n",
    "MAX_TARGET_LENGTH = 256\n",
    "GEN_MAX_NEW_TOKENS = 200\n",
    "\n",
    "# Training settings (optimized for P100 with LoRA)\n",
    "# P100 has 16GB VRAM - using conservative settings\n",
    "BATCH_SIZE = 2  # Reduced batch size for P100\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 8\n",
    "LEARNING_RATE = 1e-4  # Higher LR for LoRA\n",
    "NUM_EPOCHS = 5  # Max epochs (early stopping will likely stop earlier)\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# LoRA settings\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "# Early stopping\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "EVAL_STEPS = 20\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max input length: {MAX_INPUT_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\n",
    "print(f\"LoRA rank: {LORA_R}, alpha: {LORA_ALPHA}\")\n",
    "print(f\"Note: Settings optimized for P100 (16GB VRAM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ba5df",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4065fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "----------------------------------------\n",
      "Loading data from /home/cc/clinical-query-summarization/long-t5/data/train.json...\n",
      "  Found 392 samples\n",
      "Loading data from /home/cc/clinical-query-summarization/long-t5/data/validation.json...\n",
      "  Found 51 samples\n",
      "Loading data from /home/cc/clinical-query-summarization/long-t5/data/test.json...\n",
      "  Found 109 samples\n",
      "----------------------------------------\n",
      "Total: 392 train, 51 val, 109 test\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_json(json_path):\n",
    "    \"\"\"\n",
    "    Load clinical QA data from JSON file.\n",
    "    Each question can have multiple answers, we extract each as a separate sample.\n",
    "    \n",
    "    Input: article text + question\n",
    "    Target: answer_abs_summ (abstractive summary)\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {json_path}...\")\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    records = []\n",
    "    for qid, payload in raw_data.items():\n",
    "        question = (payload.get('question') or '').strip()\n",
    "        answers = payload.get('answers', {})\n",
    "        \n",
    "        for aid, answer_payload in answers.items():\n",
    "            article = (answer_payload.get('article') or '').strip()\n",
    "            summary = (answer_payload.get('answer_abs_summ') or '').strip()\n",
    "            \n",
    "            # Skip if article or summary is missing\n",
    "            if not article or not summary:\n",
    "                continue\n",
    "            \n",
    "            # Create the instruction prompt\n",
    "            prompt = (\n",
    "                \"Summarize the following medical article to answer the clinical question.\\n\"\n",
    "                f\"Question: {question}\\n\"\n",
    "                f\"Article: {article}\"\n",
    "            )\n",
    "            \n",
    "            records.append({\n",
    "                'id': f'{qid}_{aid}',\n",
    "                'question': question,\n",
    "                'article': article,\n",
    "                'prompt': prompt,\n",
    "                'summary': summary,\n",
    "            })\n",
    "    \n",
    "    print(f\"  Found {len(records)} samples\")\n",
    "    return records\n",
    "\n",
    "\n",
    "# Load all splits\n",
    "print(\"Loading datasets...\")\n",
    "print(\"-\" * 40)\n",
    "train_records = load_data_from_json(DATA_DIR / \"train.json\")\n",
    "val_records = load_data_from_json(DATA_DIR / \"validation.json\")\n",
    "test_records = load_data_from_json(DATA_DIR / \"test.json\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total: {len(train_records)} train, {len(val_records)} val, {len(test_records)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fd7433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training example:\n",
      "============================================================\n",
      "ID: 133_133_Answer2\n",
      "Question: how much oxazepam could cause an overdose?...\n",
      "Article length: 3216 chars\n",
      "Summary: Oxazepam is used to treat anxiety and symptoms of alcohol withdrawal. If you or some you are with overdoses, call your local emergency number, such as 911, or call your local poison center which can b...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a sample to verify the data looks correct\n",
    "print(\"Sample training example:\")\n",
    "print(\"=\" * 60)\n",
    "sample = train_records[0]\n",
    "print(f\"ID: {sample['id']}\")\n",
    "print(f\"Question: {sample['question'][:100]}...\")\n",
    "print(f\"Article length: {len(sample['article'])} chars\")\n",
    "print(f\"Summary: {sample['summary'][:200]}...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6b527",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bde6155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from google/long-t5-tglobal-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded! Vocab size: 32100\n",
      "\n",
      "Loading base model from google/long-t5-tglobal-base...\n",
      "This may take a minute...\n",
      "Model loaded!\n",
      "Total parameters: 247,587,456\n",
      "Model loaded!\n",
      "Total parameters: 247,587,456\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded! Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Load base model (we'll use this for baseline inference first)\n",
    "print(f\"\\nLoading base model from {MODEL_NAME}...\")\n",
    "print(\"This may take a minute...\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in base_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc7d2c",
   "metadata": {},
   "source": [
    "## 6. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bc556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HuggingFace datasets...\n",
      "Tokenizing datasets...\n",
      "  Tokenizing train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 392/392 [00:00<00:00, 559.64 examples/s]\n",
      "Map: 100%|██████████| 392/392 [00:00<00:00, 559.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 51/51 [00:00<00:00, 404.38 examples/s]\n",
      "Map: 100%|██████████| 51/51 [00:00<00:00, 404.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 109/109 [00:00<00:00, 529.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization complete!\n",
      "Train: 392, Val: 51, Test: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize inputs and targets for seq2seq training.\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples['prompt'],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        examples['summary'],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "print(\"Creating HuggingFace datasets...\")\n",
    "train_dataset = Dataset.from_list(train_records)\n",
    "val_dataset = Dataset.from_list(val_records)\n",
    "test_dataset = Dataset.from_list(test_records)\n",
    "\n",
    "# Tokenize\n",
    "print(\"Tokenizing datasets...\")\n",
    "print(\"  Tokenizing train set...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "print(\"  Tokenizing validation set...\")\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "print(\"  Tokenizing test set...\")\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "print(f\"\\nTokenization complete!\")\n",
    "print(f\"Train: {len(tokenized_train)}, Val: {len(tokenized_val)}, Test: {len(tokenized_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1cccb",
   "metadata": {},
   "source": [
    "## 7. Baseline Inference (Before Fine-tuning)\n",
    "\n",
    "Generate predictions with the pre-trained Long-T5 model (no fine-tuning) to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39e89542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, records, batch_size=2):\n",
    "    \"\"\"\n",
    "    Generate summaries for a list of records using the model.\n",
    "    Returns list of predictions with id, question, reference, and prediction.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    \n",
    "    total_batches = (len(records) + batch_size - 1) // batch_size\n",
    "    print(f\"Generating predictions for {len(records)} samples...\")\n",
    "    \n",
    "    for i in range(0, len(records), batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        if batch_num % 10 == 0 or batch_num == total_batches:\n",
    "            print(f\"  Processing batch {batch_num}/{total_batches}...\")\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            [r['prompt'] for r in batch],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate with P100-compatible settings\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
    "                num_beams=4,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True,\n",
    "                do_sample=False  # Greedy with beam search\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        for record, pred in zip(batch, decoded):\n",
    "            predictions.append({\n",
    "                'id': record['id'],\n",
    "                'question': record['question'],\n",
    "                'reference': record['summary'],\n",
    "                'prediction': pred.strip()\n",
    "            })\n",
    "        \n",
    "        # Clear CUDA cache periodically to prevent OOM\n",
    "        if device.type == 'cuda' and batch_num % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"  Done! Generated {len(predictions)} predictions\")\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def save_predictions(predictions, output_path):\n",
    "    \"\"\"Save predictions to JSON file.\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(f\"Saved predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982802a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing baseline predictions (skip if you want to regenerate)\n",
    "# Run this cell instead of the baseline inference cells if you already have predictions saved\n",
    "\n",
    "baseline_preds_exist = (OUTPUT_DIR / \"baseline_validation_predictions.json\").exists() and \\\n",
    "                       (OUTPUT_DIR / \"baseline_test_predictions.json\").exists()\n",
    "\n",
    "if baseline_preds_exist:\n",
    "    print(\"Loading existing baseline predictions from disk...\")\n",
    "    with open(OUTPUT_DIR / \"baseline_validation_predictions.json\", 'r') as f:\n",
    "        baseline_val_preds = json.load(f)\n",
    "    with open(OUTPUT_DIR / \"baseline_test_predictions.json\", 'r') as f:\n",
    "        baseline_test_preds = json.load(f)\n",
    "    print(f\"  Loaded {len(baseline_val_preds)} validation predictions\")\n",
    "    print(f\"  Loaded {len(baseline_test_preds)} test predictions\")\n",
    "    print(\"Baseline predictions loaded! You can skip the baseline inference cells.\")\n",
    "else:\n",
    "    print(\"No existing baseline predictions found.\")\n",
    "    print(\"Run the baseline inference cells below to generate them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a632c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE INFERENCE (Pre-trained Long-T5, no fine-tuning)\n",
      "============================================================\n",
      "\n",
      "--- Validation Set ---\n",
      "Generating predictions for 51 samples...\n",
      "  Processing batch 10/26...\n",
      "  Processing batch 20/26...\n",
      "  Processing batch 26/26...\n",
      "  Done! Generated 51 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/baseline_validation_predictions.json\n",
      "\n",
      "--- Test Set ---\n",
      "Generating predictions for 109 samples...\n",
      "  Processing batch 10/55...\n",
      "  Processing batch 20/55...\n",
      "  Processing batch 30/55...\n",
      "  Processing batch 40/55...\n",
      "  Processing batch 50/55...\n",
      "  Processing batch 55/55...\n",
      "  Done! Generated 109 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/baseline_test_predictions.json\n",
      "\n",
      "Baseline inference complete!\n"
     ]
    }
   ],
   "source": [
    "# Generate baseline predictions\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE INFERENCE (Pre-trained Long-T5, no fine-tuning)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Validation Set ---\")\n",
    "baseline_val_preds = generate_predictions(base_model, val_records, batch_size=2)\n",
    "save_predictions(baseline_val_preds, OUTPUT_DIR / \"baseline_validation_predictions.json\")\n",
    "\n",
    "print(\"\\n--- Test Set ---\")\n",
    "baseline_test_preds = generate_predictions(base_model, test_records, batch_size=2)\n",
    "save_predictions(baseline_test_preds, OUTPUT_DIR / \"baseline_test_predictions.json\")\n",
    "\n",
    "print(\"\\nBaseline inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f438f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example baseline prediction:\n",
      "------------------------------------------------------------\n",
      "Question: subjective vertigo Can macular degeneration in only one eye cause dizziness?...\n",
      "\n",
      "Reference: Dizziness has many possible causes, including inner ear disturbance, motion sickness, medication effects, and underlying health condition, such as poor circulation, infection or injury. How long the dizziness lasts and makes you feel, its triggers and other symptoms may help  determine its cause. Yo...\n",
      "\n",
      "Prediction: Symptoms People experiencing dizziness may describe it as any of a number of sensations, such as: - A false sense of motion or spinning (vertigo) - Lightheadedness or feeling faint - Unsteadiness or a loss of balance - A feeling of floating, wooziness or heavy-headedness These feelings may be trigge...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Let's see an example baseline prediction\n",
    "print(\"Example baseline prediction:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Question: {baseline_val_preds[0]['question'][:100]}...\")\n",
    "print(f\"\\nReference: {baseline_val_preds[0]['reference'][:300]}...\")\n",
    "print(f\"\\nPrediction: {baseline_val_preds[0]['prediction'][:300]}...\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a9e39",
   "metadata": {},
   "source": [
    "## 8. Apply LoRA for Parameter-Efficient Fine-tuning\n",
    "\n",
    "LoRA (Low-Rank Adaptation) freezes the pre-trained model weights and injects trainable rank decomposition matrices, reducing trainable parameters to ~0.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7042f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fresh model for LoRA fine-tuning...\n",
      "\n",
      "Configuring LoRA...\n",
      "\n",
      "Configuring LoRA...\n",
      "\n",
      "============================================================\n",
      "LoRA Configuration Applied!\n",
      "============================================================\n",
      "trainable params: 1,769,472 || all params: 249,356,928 || trainable%: 0.7096141319161583\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "LoRA Configuration Applied!\n",
      "============================================================\n",
      "trainable params: 1,769,472 || all params: 249,356,928 || trainable%: 0.7096141319161583\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Reload fresh model for fine-tuning (we used base_model for baseline)\n",
    "print(\"Loading fresh model for LoRA fine-tuning...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Configure LoRA\n",
    "print(\"\\nConfiguring LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  # Important for encoder-decoder models\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"q\", \"v\"],  # Target query and value projections\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LoRA Configuration Applied!\")\n",
    "print(\"=\" * 60)\n",
    "model.print_trainable_parameters()\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d58c5b8",
   "metadata": {},
   "source": [
    "## 9. Configure Training with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f590321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 training enabled for P100\n",
      "\n",
      "Training configuration:\n",
      "  Effective batch size: 8\n",
      "  Max epochs: 5\n",
      "  Early stopping patience: 3 evaluations\n",
      "  Evaluation every 20 steps\n",
      "  FP16: True\n"
     ]
    }
   ],
   "source": [
    "# Data collator for seq2seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100  # Ignore padding in loss\n",
    ")\n",
    "\n",
    "# Check if we can use fp16 on P100\n",
    "# P100 supports fp16 but some operations may need fallback\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "if use_fp16:\n",
    "    print(\"FP16 training enabled for P100\")\n",
    "else:\n",
    "    print(\"Using FP32 training (CPU mode)\")\n",
    "\n",
    "# Training arguments optimized for P100 with LoRA\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR / \"checkpoints\"),\n",
    "    \n",
    "    # Batch settings (conservative for P100)\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    # Learning rate and schedule\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    \n",
    "    # Epochs and evaluation\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=EVAL_STEPS,\n",
    "    \n",
    "    # Early stopping support\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Generation settings for evaluation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    \n",
    "    # Mixed precision for P100 (fp16 supported)\n",
    "    fp16=use_fp16,\n",
    "    fp16_full_eval=False,  # Use fp32 for evaluation stability\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",  # Disable wandb etc\n",
    "    \n",
    "    # Dataloader settings\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "    dataloader_pin_memory=False,  # Disable for stability\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    early_stopping_threshold=0.0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining configuration:\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE} evaluations\")\n",
    "print(f\"  Evaluation every {EVAL_STEPS} steps\")\n",
    "print(f\"  FP16: {use_fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6d120",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b276fa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized!\n",
      "Training samples: 392\n",
      "Validation samples: 51\n",
      "Steps per epoch: 49\n"
     ]
    }
   ],
   "source": [
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "print(f\"Training samples: {len(tokenized_train)}\")\n",
    "print(f\"Validation samples: {len(tokenized_val)}\")\n",
    "print(f\"Steps per epoch: {len(tokenized_train) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f6b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Note: Early stopping will stop training if validation loss\n",
      "      doesn't improve for 3 consecutive evaluations.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 06:53, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>35.750900</td>\n",
       "      <td>40.109695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>33.653900</td>\n",
       "      <td>35.957684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>30.455200</td>\n",
       "      <td>31.563232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>27.355200</td>\n",
       "      <td>28.077654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>26.724100</td>\n",
       "      <td>25.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>24.861400</td>\n",
       "      <td>25.025927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE!\n",
      "============================================================\n",
      "Total training time: 417.59 seconds\n",
      "Final training loss: 30.2790\n"
     ]
    }
   ],
   "source": [
    "# Start training!\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Note: Early stopping will stop training if validation loss\")\n",
    "print(f\"      doesn't improve for {EARLY_STOPPING_PATIENCE} consecutive evaluations.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ef8106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter saved to /home/cc/clinical-query-summarization/long-t5/outputs/lora_adapter\n"
     ]
    }
   ],
   "source": [
    "# Save the LoRA adapter\n",
    "lora_save_path = OUTPUT_DIR / \"lora_adapter\"\n",
    "model.save_pretrained(lora_save_path)\n",
    "print(f\"LoRA adapter saved to {lora_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c41ad",
   "metadata": {},
   "source": [
    "## 11. Fine-tuned Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd81c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINE-TUNED MODEL INFERENCE\n",
      "============================================================\n",
      "\n",
      "--- Validation Set ---\n",
      "Generating predictions for 51 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/clinical-query-summarization/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 10/26...\n",
      "  Processing batch 20/26...\n",
      "  Processing batch 20/26...\n",
      "  Processing batch 26/26...\n",
      "  Processing batch 26/26...\n",
      "  Done! Generated 51 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/finetuned_validation_predictions.json\n",
      "\n",
      "--- Test Set ---\n",
      "Generating predictions for 109 samples...\n",
      "  Done! Generated 51 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/finetuned_validation_predictions.json\n",
      "\n",
      "--- Test Set ---\n",
      "Generating predictions for 109 samples...\n",
      "  Processing batch 10/55...\n",
      "  Processing batch 10/55...\n",
      "  Processing batch 20/55...\n",
      "  Processing batch 20/55...\n",
      "  Processing batch 30/55...\n",
      "  Processing batch 30/55...\n",
      "  Processing batch 40/55...\n",
      "  Processing batch 40/55...\n",
      "  Processing batch 50/55...\n",
      "  Processing batch 50/55...\n",
      "  Processing batch 55/55...\n",
      "  Processing batch 55/55...\n",
      "  Done! Generated 109 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/finetuned_test_predictions.json\n",
      "\n",
      "Fine-tuned inference complete!\n",
      "  Done! Generated 109 predictions\n",
      "Saved predictions to /home/cc/clinical-query-summarization/long-t5/outputs/finetuned_test_predictions.json\n",
      "\n",
      "Fine-tuned inference complete!\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions with fine-tuned model\n",
    "print(\"=\" * 60)\n",
    "print(\"FINE-TUNED MODEL INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Validation Set ---\")\n",
    "finetuned_val_preds = generate_predictions(model, val_records, batch_size=2)\n",
    "save_predictions(finetuned_val_preds, OUTPUT_DIR / \"finetuned_validation_predictions.json\")\n",
    "\n",
    "print(\"\\n--- Test Set ---\")\n",
    "finetuned_test_preds = generate_predictions(model, test_records, batch_size=2)\n",
    "save_predictions(finetuned_test_preds, OUTPUT_DIR / \"finetuned_test_predictions.json\")\n",
    "\n",
    "print(\"\\nFine-tuned inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57baafd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example comparison (Validation set, sample 0):\n",
      "============================================================\n",
      "Question: subjective vertigo Can macular degeneration in only one eye cause dizziness?...\n",
      "\n",
      "Reference:\n",
      "Dizziness has many possible causes, including inner ear disturbance, motion sickness, medication effects, and underlying health condition, such as poor circulation, infection or injury. How long the dizziness lasts and makes you feel, its triggers and other symptoms may help  determine its cause. Yo...\n",
      "\n",
      "Fine-tuned prediction:\n",
      "Symptoms People experiencing dizziness may describe it as any of a number of sensations, such as: - A false sense of motion or spinning (vertigo) - Lightheadedness or feeling faint - Unsteadiness or a loss of balance - A feeling of floating, wooziness or heavy-headedness These feelings may be trigge...\n",
      "\n",
      "Baseline prediction:\n",
      "Symptoms People experiencing dizziness may describe it as any of a number of sensations, such as: - A false sense of motion or spinning (vertigo) - Lightheadedness or feeling faint - Unsteadiness or a loss of balance - A feeling of floating, wooziness or heavy-headedness These feelings may be trigge...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Compare example predictions\n",
    "print(\"Example comparison (Validation set, sample 0):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {finetuned_val_preds[0]['question'][:100]}...\")\n",
    "print(f\"\\nReference:\\n{finetuned_val_preds[0]['reference'][:300]}...\")\n",
    "print(f\"\\nFine-tuned prediction:\\n{finetuned_val_preds[0]['prediction'][:300]}...\")\n",
    "print(f\"\\nBaseline prediction:\\n{baseline_val_preds[0]['prediction'][:300]}...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83447b0e",
   "metadata": {},
   "source": [
    "## 12. Evaluation with ROUGE and BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb2d09b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation metrics...\n",
      "Metrics loaded!\n",
      "Metrics loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation metrics\n",
    "print(\"Loading evaluation metrics...\")\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load('bertscore')\n",
    "print(\"Metrics loaded!\")\n",
    "\n",
    "\n",
    "def compute_metrics(predictions):\n",
    "    \"\"\"\n",
    "    Compute ROUGE and BERTScore for a list of predictions.\n",
    "    Each prediction should have 'reference' and 'prediction' keys.\n",
    "    \"\"\"\n",
    "    refs = [p['reference'] for p in predictions]\n",
    "    preds = [p['prediction'] for p in predictions]\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    rouge_result = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    \n",
    "    # Compute BERTScore\n",
    "    bert_result = bertscore.compute(predictions=preds, references=refs, lang='en')\n",
    "    \n",
    "    metrics = {\n",
    "        'rouge1': round(rouge_result['rouge1'], 4),\n",
    "        'rouge2': round(rouge_result['rouge2'], 4),\n",
    "        'rougeL': round(rouge_result['rougeL'], 4),\n",
    "        'rougeLsum': round(rouge_result['rougeLsum'], 4),\n",
    "        'bertscore_precision': round(np.mean(bert_result['precision']), 4),\n",
    "        'bertscore_recall': round(np.mean(bert_result['recall']), 4),\n",
    "        'bertscore_f1': round(np.mean(bert_result['f1']), 4),\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb09d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Baseline Model (Validation Set) ---\")\n",
    "baseline_val_metrics = compute_metrics(baseline_val_preds)\n",
    "for k, v in baseline_val_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n--- Baseline Model (Test Set) ---\")\n",
    "baseline_test_metrics = compute_metrics(baseline_test_preds)\n",
    "for k, v in baseline_test_metrics.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model\n",
    "print(\"\\n--- Fine-tuned Model (Validation Set) ---\")\n",
    "finetuned_val_metrics = compute_metrics(finetuned_val_preds)\n",
    "for k, v in finetuned_val_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n--- Fine-tuned Model (Test Set) ---\")\n",
    "finetuned_test_metrics = compute_metrics(finetuned_test_preds)\n",
    "for k, v in finetuned_test_metrics.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcac96",
   "metadata": {},
   "source": [
    "## 13. Summary Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21a94fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS COMPARISON (Test Set)\n",
      "================================================================================\n",
      "Metric                           Baseline      Fine-tuned     Improvement\n",
      "--------------------------------------------------------------------------------\n",
      "rouge1                             0.2472          0.2685 +        0.0213\n",
      "rouge2                             0.0884          0.1011 +        0.0127\n",
      "rougeL                             0.1618          0.1794 +        0.0176\n",
      "rougeLsum                          0.1618          0.1792 +        0.0174\n",
      "bertscore_f1                       0.8366          0.8422 +        0.0056\n",
      "================================================================================\n",
      "\n",
      "Note: Positive improvement means fine-tuned model is better.\n"
     ]
    }
   ],
   "source": [
    "# Create comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS COMPARISON (Test Set)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Metric':<25} {'Baseline':>15} {'Fine-tuned':>15} {'Improvement':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'bertscore_f1']:\n",
    "    baseline_val = baseline_test_metrics[metric]\n",
    "    finetuned_val = finetuned_test_metrics[metric]\n",
    "    improvement = finetuned_val - baseline_val\n",
    "    sign = \"+\" if improvement > 0 else \"\"\n",
    "    print(f\"{metric:<25} {baseline_val:>15.4f} {finetuned_val:>15.4f} {sign}{improvement:>14.4f}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNote: Positive improvement means fine-tuned model is better.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e85aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All metrics saved to /home/cc/clinical-query-summarization/long-t5/outputs/evaluation_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Save all metrics to a JSON file for reference\n",
    "all_metrics = {\n",
    "    'baseline_validation': baseline_val_metrics,\n",
    "    'baseline_test': baseline_test_metrics,\n",
    "    'finetuned_validation': finetuned_val_metrics,\n",
    "    'finetuned_test': finetuned_test_metrics,\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / \"evaluation_metrics.json\", 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "print(f\"\\nAll metrics saved to {OUTPUT_DIR / 'evaluation_metrics.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860393a",
   "metadata": {},
   "source": [
    "## 14. Output Files Summary\n",
    "\n",
    "The following files have been generated:\n",
    "\n",
    "```\n",
    "outputs/\n",
    "├── baseline_validation_predictions.json   # Baseline model predictions on validation set\n",
    "├── baseline_test_predictions.json         # Baseline model predictions on test set\n",
    "├── finetuned_validation_predictions.json  # Fine-tuned model predictions on validation set\n",
    "├── finetuned_test_predictions.json        # Fine-tuned model predictions on test set\n",
    "├── evaluation_metrics.json                # All ROUGE and BERTScore metrics\n",
    "├── lora_adapter/                          # Saved LoRA weights (can be loaded later)\n",
    "└── checkpoints/                           # Training checkpoints\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "print(\"Generated output files:\")\n",
    "print(\"-\" * 40)\n",
    "for f in OUTPUT_DIR.iterdir():\n",
    "    if f.is_file():\n",
    "        size = f.stat().st_size / 1024  # KB\n",
    "        print(f\"  {f.name}: {size:.1f} KB\")\n",
    "    else:\n",
    "        print(f\"  {f.name}/ (directory)\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\nDone! Notebook execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
