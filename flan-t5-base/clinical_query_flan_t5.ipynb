{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1cafb7",
   "metadata": {},
   "source": [
    "# FLAN-T5 Clinical Query Summaries\n",
    "This notebook fine-tunes `google/flan-t5-base` on the clinical query summarization dataset under `data/`. It also captures baseline generations from the frozen base model and evaluates everything with ROUGE + BERTScore so you can compare against the artifacts in `gemma_2/outputs`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0087f33",
   "metadata": {},
   "source": [
    "## Notebook game plan\n",
    "1. Install/load dependencies and set up paths.\n",
    "2. Parse the provided JSON splits into flattened instruction/target rows.\n",
    "3. Tokenize data for FLAN-T5 and configure the seq2seq trainer.\n",
    "4. Fine-tune, evaluate, and save generated summaries plus metrics.\n",
    "5. Run baseline inference with the untouched FLAN-T5-Base model and dump predictions to JSON for side-by-side comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"transformers>=4.44.0\" \"datasets>=2.19.0\" \"evaluate>=0.4.1\" \"accelerate>=0.30.0\" sentencepiece rouge-score bert-score\n",
    "\n",
    "print(\"âœ… Installed core NLP libraries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "MAX_INPUT_TOKENS = 1024\n",
    "MAX_TARGET_TOKENS = 256\n",
    "SEED = 7\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ“‚ Data dir: {DATA_DIR.resolve()}\")\n",
    "print(f\"ðŸ’¾ Outputs will land in: {OUTPUT_DIR.resolve()}\")\n",
    "print(f\"ðŸ§  Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split_path: Path):\n",
    "    with open(split_path) as fp:  # JSON file fits in memory\n",
    "        payload = json.load(fp)\n",
    "    rows = []\n",
    "    for qid, q_payload in payload.items():\n",
    "        question = (q_payload.get(\"question\") or \"\").strip()\n",
    "        answers = q_payload.get(\"answers\", {})\n",
    "        for aid, ans_payload in answers.items():\n",
    "            article = (ans_payload.get(\"article\") or \"\").strip()\n",
    "            target = (ans_payload.get(\"answer_abs_summ\") or \"\").strip()\n",
    "            if not article or not target:\n",
    "                continue  # skip incomplete rows\n",
    "            prompt = (\n",
    "                \"Summarize the following clinical article to answer the patient question.\\n\"\n",
    "                f\"Question: {question}\\n\\n\"\n",
    "                f\"Article:\\n{article}\"\n",
    "            ).strip()\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"uid\": f\"{qid}_{aid}\",\n",
    "                    \"question\": question,\n",
    "                    \"article\": article,\n",
    "                    \"input_text\": prompt,\n",
    "                    \"target_text\": target,\n",
    "                }\n",
    "            )\n",
    "    return rows\n",
    "\n",
    "splits = {}\n",
    "for name in [\"train\", \"validation\", \"test\"]:\n",
    "    split_path = DATA_DIR / f\"{name}.json\"\n",
    "    split_rows = load_split(split_path)\n",
    "    splits[name] = split_rows\n",
    "    print(f\"{name.title()} examples: {len(split_rows)}\")\n",
    "\n",
    "dataset_dict = DatasetDict({k: Dataset.from_list(v) for k, v in splits.items()})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd236650",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = dataset_dict[\"train\"][0]\n",
    "for key in [\"uid\", \"question\", \"input_text\", \"target_text\"]:\n",
    "    preview = sample_row[key][:500]\n",
    "    suffix = \"...\" if len(sample_row[key]) > len(preview) else \"\"\n",
    "    print(f\"\\n--- {key} ---\")\n",
    "    print(preview + suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer max length: {tokenizer.model_max_length}\")\n",
    "\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        max_length=MAX_INPUT_TOKENS,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        batch[\"target_text\"],\n",
    "        max_length=MAX_TARGET_TOKENS,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    label_ids = []\n",
    "    for input_ids, attn_mask in zip(labels[\"input_ids\"], labels[\"attention_mask\"]):\n",
    "        clean_ids = [token if mask == 1 else -100 for token, mask in zip(input_ids, attn_mask)]\n",
    "        label_ids.append(clean_ids)\n",
    "\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "remove_cols = dataset_dict[\"train\"].column_names\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=remove_cols,\n",
    "    desc=\"Tokenizing dataset\",\n",
    ")\n",
    "\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e04191",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = np.asarray(preds)\n",
    "    if preds.ndim == 3:  # logits â†’ token ids\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "    preds = preds.astype(np.int64, copy=False)\n",
    "    preds = np.where(preds < 0, tokenizer.pad_token_id, preds)\n",
    "    decoded_preds = tokenizer.batch_decode(preds.tolist(), skip_special_tokens=True)\n",
    "\n",
    "    if isinstance(labels, tuple):\n",
    "        labels = labels[0]\n",
    "    labels = np.asarray(labels, dtype=np.int64)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels.tolist(), skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    result = {k: round(v, 4) for k, v in rouge_result.items()}\n",
    "    result[\"prediction_len\"] = np.mean(\n",
    "        [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31166912",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "label_smoothing = 0.1\n",
    "\n",
    "import inspect\n",
    "\n",
    "training_kwargs = dict(\n",
    "    output_dir=str(OUTPUT_DIR / \"flan_t5_finetuned\"),\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_TOKENS,\n",
    "    report_to=[\"none\"],\n",
    "    seed=SEED,\n",
    "    label_smoothing_factor=label_smoothing,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeLsum\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "training_signature = inspect.signature(Seq2SeqTrainingArguments.__init__)\n",
    "if \"evaluation_strategy\" in training_signature.parameters:\n",
    "    training_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "elif \"eval_strategy\" in training_signature.parameters:\n",
    "    training_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "else:\n",
    "    print(\"âš ï¸ evaluation_strategy not supported in this transformers version; defaulting to no periodic eval.\")\n",
    "\n",
    "if \"fp16\" in training_signature.parameters:\n",
    "    training_kwargs[\"fp16\"] = torch.cuda.is_available()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(**training_kwargs)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()\n",
    "print(\"Training complete âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09710e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"], metric_key_prefix=\"eval\")\n",
    "trainer.save_metrics(\"eval\", validation_metrics)\n",
    "print(\"Validation metrics:\", validation_metrics)\n",
    "\n",
    "if \"test\" in tokenized_datasets:\n",
    "    test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"], metric_key_prefix=\"test\")\n",
    "    trainer.save_metrics(\"test\", test_metrics)\n",
    "    print(\"Test metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e615413",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_DIR = OUTPUT_DIR / \"flan_t5_predictions\"\n",
    "PREDICTION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def chunk_list(items, size):\n",
    "    for idx in range(0, len(items), size):\n",
    "        yield items[idx : idx + size]\n",
    "\n",
    "\n",
    "def generate_and_save_predictions(model, tokenizer, records, split_name, output_path, batch_size=4, max_new_tokens=MAX_TARGET_TOKENS):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    rows = []\n",
    "    for batch in chunk_list(records, batch_size):\n",
    "        inputs = tokenizer(\n",
    "            [row[\"input_text\"] for row in batch],\n",
    "            max_length=MAX_INPUT_TOKENS,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_beams=4,\n",
    "                length_penalty=1.0,\n",
    "            )\n",
    "        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        for row, pred in zip(batch, decoded):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"uid\": row[\"uid\"],\n",
    "                    \"question\": row[\"question\"],\n",
    "                    \"article\": row[\"article\"],\n",
    "                    \"reference\": row[\"target_text\"],\n",
    "                    \"prediction\": pred.strip(),\n",
    "                }\n",
    "            )\n",
    "    with open(output_path, \"w\") as fp:\n",
    "        json.dump(rows, fp, indent=2)\n",
    "    print(f\"Saved {len(rows)} predictions â†’ {output_path}\")\n",
    "\n",
    "\n",
    "FINETUNED_VAL_PATH = PREDICTION_DIR / \"finetuned_validation_predictions.json\"\n",
    "FINETUNED_TEST_PATH = PREDICTION_DIR / \"finetuned_test_predictions.json\"\n",
    "\n",
    "generate_and_save_predictions(trainer.model, tokenizer, splits[\"validation\"], \"validation\", FINETUNED_VAL_PATH)\n",
    "generate_and_save_predictions(trainer.model, tokenizer, splits[\"test\"], \"test\", FINETUNED_TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebe340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_text_metrics(pred_path, prefix):\n",
    "    with open(pred_path) as fp:\n",
    "        payload = json.load(fp)\n",
    "    predictions = [row[\"prediction\"] for row in payload]\n",
    "    references = [row[\"reference\"] for row in payload]\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    metrics = {f\"{prefix}_\" + k: round(v, 4) for k, v in rouge_scores.items()}\n",
    "    metrics.update(\n",
    "        {\n",
    "            f\"{prefix}_bertscore_precision\": float(np.mean(bert_scores[\"precision\"])),\n",
    "            f\"{prefix}_bertscore_recall\": float(np.mean(bert_scores[\"recall\"])),\n",
    "            f\"{prefix}_bertscore_f1\": float(np.mean(bert_scores[\"f1\"])),\n",
    "        }\n",
    "    )\n",
    "    return metrics\n",
    "\n",
    "finetuned_val_metrics = compute_text_metrics(FINETUNED_VAL_PATH, \"finetuned_val\")\n",
    "finetuned_test_metrics = compute_text_metrics(FINETUNED_TEST_PATH, \"finetuned_test\")\n",
    "\n",
    "print(\"Finetuned validation metrics:\", json.dumps(finetuned_val_metrics, indent=2))\n",
    "print(\"Finetuned test metrics:\", json.dumps(finetuned_test_metrics, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b02a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_DIR = OUTPUT_DIR / \"flan_t5_baseline\"\n",
    "BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "baseline_model.to(device)\n",
    "baseline_model.eval()\n",
    "\n",
    "BASELINE_VAL_PATH = BASELINE_DIR / \"baseline_validation_predictions.json\"\n",
    "BASELINE_TEST_PATH = BASELINE_DIR / \"baseline_test_predictions.json\"\n",
    "\n",
    "generate_and_save_predictions(baseline_model, tokenizer, splits[\"validation\"], \"validation\", BASELINE_VAL_PATH)\n",
    "generate_and_save_predictions(baseline_model, tokenizer, splits[\"test\"], \"test\", BASELINE_TEST_PATH)\n",
    "\n",
    "baseline_val_metrics = compute_text_metrics(BASELINE_VAL_PATH, \"baseline_val\")\n",
    "baseline_test_metrics = compute_text_metrics(BASELINE_TEST_PATH, \"baseline_test\")\n",
    "\n",
    "print(\"Baseline validation metrics:\", json.dumps(baseline_val_metrics, indent=2))\n",
    "print(\"Baseline test metrics:\", json.dumps(baseline_test_metrics, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01dedb7",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Inspect the JSON prediction files saved under `outputs/` for downstream analysis or comparison with Gemma-2 results.\n",
    "- Swap out hyperparameters (epochs, LR, max tokens) and re-run the fine-tuning cell to explore trade-offs.\n",
    "- If you only need inference, skip the training cell and run the baseline generation cell directly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
