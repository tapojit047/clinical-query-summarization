{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c71af7",
   "metadata": {},
   "source": [
    "# T5-Small Instruction Fine-Tuning for Clinical Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4e3e8",
   "metadata": {},
   "source": [
    "Fine-tune Hugging Face `t5-small` on the custom clinical QA dataset under `t5-small/data`.\n",
    "The notebook also records zero-shot baselines, evaluates with ROUGE + BERTScore, and saves JSON predictions for later review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0cf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets evaluate accelerate bert-score rouge-score\n",
    "print(\"Finished installing the project dependencies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b03bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "BASE_DIR = Path('t5-small')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "PRED_DIR = OUTPUT_DIR / 'predictions'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = 't5-small'\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 256\n",
    "GEN_MAX_NEW_TOKENS = 160\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'Data directory: {DATA_DIR.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    records = []\n",
    "    for qid, payload in raw_data.items():\n",
    "        question = (payload.get('question') or '').strip()\n",
    "        answers = payload.get('answers', {})\n",
    "        for aid, answer_payload in answers.items():\n",
    "            article = (answer_payload.get('article') or '').strip()\n",
    "            summary = (answer_payload.get('answer_abs_summ') or '').strip()\n",
    "            if not article or not summary:\n",
    "                continue\n",
    "            prompt = (\n",
    "                \"Summarize the following medical article to answer the clinical question.\\n\"\n",
    "                f\"Question: {question}\\n\"\n",
    "                f\"Article: {article}\"\n",
    "            )\n",
    "            records.append(\n",
    "                {\n",
    "                    'id': f'{qid}_{aid}',\n",
    "                    'question': question,\n",
    "                    'article': article,\n",
    "                    'prompt': ''.join(prompt),\n",
    "+        \n",
    ",\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = load_split(DATA_DIR / 'train.json')\n",
    "val_records = load_split(DATA_DIR / 'validation.json')\n",
    "test_records = load_split(DATA_DIR / 'test.json')\n",
    "print(f'Train/Val/Test sizes -> {len(train_records)} / {len(val_records)} / {len(test_records)}')\n",
    "\n",
    "def peek(records, name):\n",
    "    if not records:\n",
    "        print(f'No records available for {name}.')\n",
    "        return\n",
    "    print(f\"\\n{name} sample prompt (first 300 chars):\\n{records[0]['prompt'][:300]}...\")\n",
    "    print(f\"{name} sample reference:\\n{records[0]['summary']}\\n\")\n",
    "\n",
    "peek(train_records, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print('Tokenizer loaded.')\n",
    "\n",
    "hf_train = Dataset.from_list(train_records)\n",
    "hf_val = Dataset.from_list(val_records)\n",
    "hf_test = Dataset.from_list(test_records)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch['prompt'],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        batch['summary'],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = hf_train.map(tokenize_batch, batched=True, remove_columns=hf_train.column_names, desc='Tokenizing train')\n",
    "tokenized_val = hf_val.map(tokenize_batch, batched=True, remove_columns=hf_val.column_names, desc='Tokenizing val')\n",
    "tokenized_test = hf_test.map(tokenize_batch, batched=True, remove_columns=hf_test.column_names, desc='Tokenizing test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a814474",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load('bertscore')\n",
    "print('Loaded ROUGE and BERTScore evaluators.')\n",
    "\n",
    "def compute_text_metrics(preds, refs):\n",
    "    rouge_result = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    bert_result = bertscore.compute(predictions=preds, references=refs, lang='en')\n",
    "    metrics = {f'rouge_{k}': round(v, 4) for k, v in rouge_result.items()}\n",
    "    metrics['bertscore_f1'] = float(np.mean(bert_result['f1']))\n",
    "    return metrics\n",
    "\n",
    "def run_batch_generation(model, records, split_name, output_path, batch_size=4):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    iterator = range(0, len(records), batch_size)\n",
    "    for start in tqdm(iterator, desc=f'Generating {split_name}', leave=False):\n",
    "        batch = records[start:start + batch_size]\n",
    "        if not batch:\n",
    "            continue\n",
    "        inputs = tokenizer(\n",
    "            [row['prompt'] for row in batch],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
    "                num_beams=4,\n",
    "                length_penalty=1.0,\n",
    "            )\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        for row, pred in zip(batch, decoded):\n",
    "            predictions.append(\n",
    "                {\n",
    "                    'id': row['id'],\n",
    "                    'question': row['question'],\n",
    "                    'prediction': pred.strip(),\n",
    "                    'reference': row['summary'],\n",
    "                }\n",
    "            )\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(f'Saved {len(predictions)} predictions to {output_path}')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "print('Running baseline generation on validation data...')\n",
    "baseline_val_results = run_batch_generation(\n",
    "    baseline_model,\n",
    "    val_records,\n",
    "    'baseline-val',\n",
    "    PRED_DIR / 'baseline_val_predictions.json'\n",
    ")\n",
    "baseline_val_metrics = compute_text_metrics(\n",
    "    [row['prediction'] for row in baseline_val_results],\n",
    "    [row['reference'] for row in baseline_val_results]\n",
    ")\n",
    "print('Baseline validation metrics:', baseline_val_metrics)\n",
    "\n",
    "print('Running baseline generation on test data...')\n",
    "baseline_test_results = run_batch_generation(\n",
    "    baseline_model,\n",
    "    test_records,\n",
    "    'baseline-test',\n",
    "    PRED_DIR / 'baseline_test_predictions.json'\n",
    ")\n",
    "baseline_test_metrics = compute_text_metrics(\n",
    "    [row['prediction'] for row in baseline_test_results],\n",
    "    [row['reference'] for row in baseline_test_results]\n",
    ")\n",
    "print('Baseline test metrics:', baseline_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=finetune_model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR / 't5_small_finetune'),\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=GEN_MAX_NEW_TOKENS,\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "def trainer_compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return compute_text_metrics(decoded_preds, decoded_labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=trainer_compute_metrics,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR / 't5_small_finetune')\n",
    "tokenizer.save_pretrained(OUTPUT_DIR / 't5_small_finetune')\n",
    "print('Training finished and model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(sequences):\n",
    "    decoded = tokenizer.batch_decode(sequences, skip_special_tokens=True)\n",
    "    return [text.strip() for text in decoded]\n",
    "\n",
    "val_predictions = trainer.predict(tokenized_val)\n",
    "val_decoded_preds = decode_sequences(val_predictions.predictions)\n",
    "val_labels = np.where(val_predictions.label_ids == -100, tokenizer.pad_token_id, val_predictions.label_ids)\n",
    "val_decoded_refs = decode_sequences(val_labels)\n",
    "finetuned_val_metrics = compute_text_metrics(val_decoded_preds, val_decoded_refs)\n",
    "print('Fine-tuned validation metrics:', finetuned_val_metrics)\n",
    "\n",
    "finetuned_val_results = []\n",
    "for record, pred, ref in zip(val_records, val_decoded_preds, val_decoded_refs):\n",
    "    finetuned_val_results.append(\n",
    "        {\n",
    "            'id': record['id'],\n",
    "            'question': record['question'],\n",
    "            'prediction': pred,\n",
    "            'reference': ref,\n",
    "        }\n",
    "    )\n",
    "with open(PRED_DIR / 'finetuned_val_predictions.json', 'w') as f:\n",
    "    json.dump(finetuned_val_results, f, indent=2)\n",
    "print('Saved fine-tuned validation predictions.')\n",
    "\n",
    "test_predictions = trainer.predict(tokenized_test)\n",
    "test_decoded_preds = decode_sequences(test_predictions.predictions)\n",
    "test_labels = np.where(test_predictions.label_ids == -100, tokenizer.pad_token_id, test_predictions.label_ids)\n",
    "test_decoded_refs = decode_sequences(test_labels)\n",
    "finetuned_test_metrics = compute_text_metrics(test_decoded_preds, test_decoded_refs)\n",
    "print('Fine-tuned test metrics:', finetuned_test_metrics)\n",
    "\n",
    "finetuned_test_results = []\n",
    "for record, pred, ref in zip(test_records, test_decoded_preds, test_decoded_refs):\n",
    "    finetuned_test_results.append(\n",
    "        {\n",
    "            'id': record['id'],\n",
    "            'question': record['question'],\n",
    "            'prediction': pred,\n",
    "            'reference': ref,\n",
    "        }\n",
    "    )\n",
    "with open(PRED_DIR / 'finetuned_test_predictions.json', 'w') as f:\n",
    "    json.dump(finetuned_test_results, f, indent=2)\n",
    "print('Saved fine-tuned test predictions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_split(name, baseline_metrics, finetuned_metrics):\n",
    "    metric_keys = sorted(set(baseline_metrics) | set(finetuned_metrics))\n",
    "    print(f\"\\n{name} metrics\")\n",
    "    print(\"Metric\".ljust(20), \"Baseline\".ljust(12), \"Finetuned\".ljust(12), \"Delta\")\n",
    "    print(\"-\" * 60)\n",
    "    for key in metric_keys:\n",
    "        base_val = baseline_metrics.get(key)\n",
    "        tune_val = finetuned_metrics.get(key)\n",
    "        delta = None if (base_val is None or tune_val is None) else tune_val - base_val\n",
    "        print(\n",
    "            key.ljust(20),\n",
    "            f\"{base_val:.4f}\".ljust(12) if base_val is not None else \"--\".ljust(12),\n",
    "            f\"{tune_val:.4f}\".ljust(12) if tune_val is not None else \"--\".ljust(12),\n",
    "            f\"{delta:+.4f}\" if delta is not None else \"--\",\n",
    "        )\n",
    "\n",
    "required_vars = [\n",
    "    'baseline_val_metrics',\n",
    "    'baseline_test_metrics',\n",
    "    'finetuned_val_metrics',\n",
    "    'finetuned_test_metrics',\n",
    "]\n",
    "missing = [var for var in required_vars if var not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"Please execute the baseline and fine-tuned evaluation cells before running this comparison block.\"\n",
    "    )\n",
    "\n",
    "summarize_split('Validation', baseline_val_metrics, finetuned_val_metrics)\n",
    "summarize_split('Test', baseline_test_metrics, finetuned_test_metrics)\n",
    "print(\"\\nDone comparing baseline vs. fine-tuned performance.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
